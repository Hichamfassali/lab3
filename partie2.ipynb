{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad3c06-7b62-466d-93e7-1d2674a82f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe5bc09-0d0f-4a41-a63b-ed8428694008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea5ba72d0f242e1bca536c2d19ecf5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad3fa619c0541448891316382b3a118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f087b0d4214473696facf598bac8655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8a0caeb68548ec819d834d0ec9c625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2ea3b031af4328bbf66e53ae7fd111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad156083a391496780841d914180f2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bf5d61b62ef4eef8b07432ca5e79535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0376b8-305b-46b3-9dd6-544ea5f01920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_from_top(probs, n=5):\n",
    "    ind = np.argpartition(probs, -n)[-n:]\n",
    "    top_prob = probs[ind]\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(n, 1, p = top_prob)\n",
    "    token_id = ind[choice][0]\n",
    "    return int(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9f7d9-2cca-49bc-bdf6-517af27612b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "class JokesDataset(Dataset):\n",
    "    def __init__(self, jokes_dataset_path = 'C:/data/'):\n",
    "        super().__init__()\n",
    "\n",
    "        short_jokes_path = os.path.join(jokes_dataset_path, 'shortjokes.csv')\n",
    "\n",
    "        self.joke_list = []\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "        \n",
    "        with open(short_jokes_path) as csv_file:\n",
    "            csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "            \n",
    "            x = 0\n",
    "            for row in csv_reader:\n",
    "                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n",
    "                self.joke_list.append(joke_str)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.joke_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.joke_list[item]\n",
    "dataset = JokesDataset()\n",
    "joke_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e63ff-20d5-43d8-b39c-645d7133f822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "TAILLE_BATCH = 16\n",
    "EPOCHS = 5\n",
    "TAUX_APPRENTISSAGE = 3e-5\n",
    "ETAPES_CHAUFFAGE = 5000\n",
    "LONGUEUR_SEQ_MAX = 400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e63ebc-43d7-4bf9-b10d-fbe7afd356d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started==============================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "# Paramètres\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 5000\n",
    "MAX_SEQ_LEN = 400\n",
    "\n",
    "# Préparation du modèle et du périphérique\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Calcul du nombre total d'étapes d'entraînement\n",
    "TOTAL_TRAINING_STEPS = len(joke_loader) * EPOCHS\n",
    "\n",
    "# Initialisation du scheduler\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\", optimizer=optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=TOTAL_TRAINING_STEPS\n",
    ")\n",
    "\n",
    "# Initialisation des variables\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "tmp_jokes_tens = None\n",
    "models_folder = \"trained_models\"\n",
    "if not os.path.exists(models_folder):\n",
    "    os.mkdir(models_folder)\n",
    "\n",
    "# Entraînement\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "    \n",
    "    for idx, joke in enumerate(joke_loader):\n",
    "        \n",
    "        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
    "        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n",
    "        # Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
    "        if joke_tens.size()[1] > MAX_SEQ_LEN:\n",
    "            continue\n",
    "        \n",
    "        # The first joke sequence in the sequence\n",
    "        if not torch.is_tensor(tmp_jokes_tens):\n",
    "            tmp_jokes_tens = joke_tens\n",
    "            continue\n",
    "        else:\n",
    "            # The next joke does not fit in, so process the sequence and leave the last joke \n",
    "            # as the start for the next sequence \n",
    "            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n",
    "                work_jokes_tens = tmp_jokes_tens\n",
    "                tmp_jokes_tens = joke_tens\n",
    "            else:\n",
    "                # Add the joke to sequence, continue and try to add more\n",
    "                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n",
    "                continue\n",
    "        ################## Sequence ready, process it through the model ##################\n",
    "            \n",
    "        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n",
    "        loss = outputs.loss                        \n",
    "        loss.backward()\n",
    "        sum_loss = sum_loss + loss.detach().data\n",
    "                       \n",
    "        proc_seq_count += 1\n",
    "        if proc_seq_count == BATCH_SIZE:\n",
    "            proc_seq_count = 0    \n",
    "            batch_count += 1\n",
    "            optimizer.step()\n",
    "            scheduler.step() \n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "        if batch_count == 100:\n",
    "            print(f\"sum loss {sum_loss}\")\n",
    "            batch_count = 0\n",
    "            sum_loss = 0.0\n",
    "    \n",
    "    # Store the model after each epoch to compare the performance of them\n",
    "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_joker_{epoch}.pt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392873e-1945-42c0-8ad1-ec420453f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EPOCH = 4\n",
    "\n",
    "models_folder = \"trained_models\"\n",
    "\n",
    "model_path = os.path.join(models_folder, f\"gpt2_medium_joker_{MODEL_EPOCH}.pt\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "jokes_output_file_path = f'generated_{MODEL_EPOCH}.jokes'\n",
    "\n",
    "model.eval()\n",
    "if os.path.exists(jokes_output_file_path):\n",
    "    os.remove(jokes_output_file_path)\n",
    "    \n",
    "joke_num = 0\n",
    "with torch.no_grad():\n",
    "   \n",
    "        for joke_idx in range(1000):\n",
    "        \n",
    "            joke_finished = False\n",
    "\n",
    "            cur_ids = torch.tensor(tokenizer.encode(\"JOKE:\")).unsqueeze(0).to(device)\n",
    "\n",
    "            for i in range(100):\n",
    "                outputs = model(cur_ids, labels=cur_ids)\n",
    "                loss, logits = outputs[:2]\n",
    "                softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n",
    "                if i < 3:\n",
    "                    n = 20\n",
    "                else:\n",
    "                    n = 3\n",
    "                next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n",
    "                cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n",
    "\n",
    "                if next_token_id in tokenizer.encode('<|endoftext|>'):\n",
    "                    joke_finished = True\n",
    "                    break\n",
    "\n",
    "            \n",
    "            if joke_finished:\n",
    "                \n",
    "                joke_num = joke_num + 1\n",
    "                \n",
    "                output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
    "                output_text = tokenizer.decode(output_list)\n",
    "\n",
    "                with open(jokes_output_file_path, 'a') as f:\n",
    "                    f.write(f\"{output_text} \\n\\n\")\n",
    "                    \n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
